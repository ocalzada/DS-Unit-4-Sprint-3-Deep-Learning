{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical, get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/100/100-0.txt\n",
      "5783552/5777367 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "\n",
    "doc = get_file('shakespeare.txt', url)\n",
    "text = open(doc, 'rb').read().decode(encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740053"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[-25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Text Preprocessing'''\n",
    "\n",
    "'''Removing \\r'''\n",
    "text = text.replace('\\r', '')\n",
    "'''Making all text lower-case'''\n",
    "text = text[900:-25000].lower()\n",
    "'''Fixing spacing'''\n",
    "text = ' '.join(text.split())\n",
    "\n",
    "''' Getting all the letters/characters used in the Text '''\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "''' Enumerating all the letters/characters into ints '''\n",
    "char_to_int = {c:i for i, c in enumerate(vocab)}\n",
    "int_to_char = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "text_integers = np.array([char_to_int[c] for c in text])\n",
    "\n",
    "''' Per Epoch '''\n",
    "seq_length = 100\n",
    "\n",
    "X_text = []\n",
    "y_text = []\n",
    "\n",
    "for i in range(0, 100000 - seq_length,1):\n",
    "    in_seq = text[i:i + seq_length]\n",
    "    out_char = text[i + seq_length]\n",
    "    X_text.append([char_to_int[char] for char in in_seq])\n",
    "    y_text.append(char_to_int[out_char])\n",
    "    \n",
    "samples = len(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99900"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99900, 100, 1)\n",
      "(99900, 71)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(X_text, (99900, 100, 1))\n",
    "X = X / len(vocab)\n",
    "print(X.shape)\n",
    "y = to_categorical(y_text)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 71)                18247     \n",
      "=================================================================\n",
      "Total params: 807,751\n",
      "Trainable params: 807,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' Building The Model '''\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "''' Building The Model '''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99900 samples\n",
      "Epoch 1/50\n",
      "99900/99900 [==============================] - 19s 194us/sample - loss: 2.4963\n",
      "Epoch 2/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.4730\n",
      "Epoch 3/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.4535\n",
      "Epoch 4/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.4364\n",
      "Epoch 5/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.4223\n",
      "Epoch 6/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3976\n",
      "Epoch 7/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3818\n",
      "Epoch 8/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3602\n",
      "Epoch 9/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3419\n",
      "Epoch 10/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3218\n",
      "Epoch 11/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3006\n",
      "Epoch 12/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 2.2807\n",
      "Epoch 13/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.2614\n",
      "Epoch 14/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 2.2432\n",
      "Epoch 15/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.2290\n",
      "Epoch 16/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.2140\n",
      "Epoch 17/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1960\n",
      "Epoch 18/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1761\n",
      "Epoch 19/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1618\n",
      "Epoch 20/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1475\n",
      "Epoch 21/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 2.1310\n",
      "Epoch 22/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1141\n",
      "Epoch 23/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0964\n",
      "Epoch 24/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0851\n",
      "Epoch 25/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0715\n",
      "Epoch 26/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0550\n",
      "Epoch 27/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0402\n",
      "Epoch 28/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0270\n",
      "Epoch 29/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 2.0093\n",
      "Epoch 30/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9951\n",
      "Epoch 31/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9843\n",
      "Epoch 32/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9639\n",
      "Epoch 33/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9505\n",
      "Epoch 34/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9407\n",
      "Epoch 35/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9228\n",
      "Epoch 36/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9113\n",
      "Epoch 37/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8947\n",
      "Epoch 38/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8814\n",
      "Epoch 39/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8647\n",
      "Epoch 40/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8537\n",
      "Epoch 41/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8401\n",
      "Epoch 42/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8279\n",
      "Epoch 43/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8122\n",
      "Epoch 44/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8027\n",
      "Epoch 45/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7852\n",
      "Epoch 46/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7746\n",
      "Epoch 47/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7656\n",
      "Epoch 48/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 1.7455\n",
      "Epoch 49/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7339\n",
      "Epoch 50/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metric=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X, y, batch_size=1000, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \n",
      " my joy behind. 51 thus can my love excuse the slow offence, of my dull bearer, when from thee i spee\n",
      "\n",
      "\n",
      "LSTM Generated Text OH MY:\n",
      "\n",
      "my joy behind. 51 thus can my love excuse the slow offence, of my dull bearer,\n",
      "when from thee i speer becined, and thet the thme doth lake me song the stage\n",
      "and shen the strengnt of the sime of the were oor of thee, and thou art all the\n",
      "wirl of thee ae oot, oor that i do doth pene. that thou thall beauty should mote\n",
      "beligte. then thet be beauty of the seaond stars of the wirl of thee, and the\n",
      "dear feart’s palace. scene ii. tossillon. a room in the countess’s palace. scene\n",
      "ii. tossillon. a room in the countess’s palace. scene ii. tossillon. a room in\n",
      "the countess’s palace. scene ii. tossillon.\n"
     ]
    }
   ],
   "source": [
    "''' Generate Text '''\n",
    "import textwrap\n",
    "\n",
    "start = np.random.randint(0, len(X_text)-1)\n",
    "vocab_len = len(vocab)\n",
    "pattern = X_text[start]\n",
    "\n",
    "print(f\"Seed: \\n {''.join([int_to_char[value] for value in pattern])}\")\n",
    "out = [int_to_char[value] for value in pattern]\n",
    "\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    in_seq = [int_to_char[value] for value in pattern]\n",
    "    out.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print('\\n')\n",
    "print(\"LSTM Generated Text OH MY:\\n\")\n",
    "print(textwrap.fill(''.join(out), 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c,i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contents the sonnets all’s well that ends well the'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 1751469\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print('sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contents the sonnets all’s well that end'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chars[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1751469, 40, 71)\n",
      "(1751469, 71)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metric='acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1751469 samples\n",
      "Epoch 1/5\n",
      "1750656/1751469 [============================>.] - ETA: 0s - loss: 1.5426\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"luellen. your grace does me as great hon\"\n",
      "luellen. your grace does me as great honour of the soul to the thing in the more the shall the sons that the bears the soul the sons of the shall the courtest bear the more the sons. come, he hath the stand the sun the death of the love to the the thing of the duke of the as the strange and bear the hour in the servant of the heaven of the sing of the hearts of the death the world to the servant of the stand that shall say the servant t\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"luellen. your grace does me as great hon\"\n",
      "luellen. your grace does me as great honourable bear the beard the meaning to the most a morning the hearts to make the soul more that it was thine end the sard that have serve the fair of the bear ere shall be hang the proud to the return in the sorry and lord, and done, and here, thou spite, and the never with all the bears of your courts in the bear but the shake the second there doth mistaven that shall deliver the poster. falstaff.\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"luellen. your grace does me as great hon\"\n",
      "luellen. your grace does me as great honour; ihus. perinius. easre! lac'd, othering are should then, dost not marther. king proteus. fensing. servant. whir that, till very roos my most alming not the wis all thou will we belieged hagk and the letter', and fully will to flow, and coff too you. but life my mastines a most thy duke, the pear'st shall, i that thy by well outshipes in made ones, shall need them'st suitions and daughter, isfe\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"luellen. your grace does me as great hon\"\n",
      "luellen. your grace does me as great honeywasks begins so e beent o greek'd you, who now. fal you grews immew both atterele. no manious. if so he from anonish tutper. if would tell thyself. _evols, known this hoaring. wiunt ready hope; she cifbion therebufi'd, should treasure, on hatfur man have yong’d, capiter, say much doth tid oghortaty well to nirmarance. if ourselfsavius to tell o' yourstry say; iachate to ketking her delad’d, but \n",
      "1751469/1751469 [==============================] - 134s 77us/sample - loss: 1.5426\n",
      "Epoch 2/5\n",
      "1751424/1751469 [============================>.] - ETA: 0s - loss: 1.5219\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"xenes. o, father, you’ll know more of th\"\n",
      "xenes. o, father, you’ll know more of the country with thee shall be his should be a court of the many the earth the parting and the seas and the man that i have hear thee shall be the many the seas, and the man of the great a servants and a servants and see the man shall be the man the man that will be a straight of the man the seas of the part of a man to see the seas unto the suffer the man that is the true and the parting that will \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"xenes. o, father, you’ll know more of th\"\n",
      "xenes. o, father, you’ll know more of the good ascall and thus be with it stain, and for my man in thee; and the sum for my father's place them with melancholy lives them that may think a fingers of a man that is a countay thee, he be doth matter of this good grand soldier and beauty to straon there is not the sent in the been from you, and in the kind will never be speak and are have her as shall see thee in thy hands pard a doubles th\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"xenes. o, father, you’ll know more of th\"\n",
      "xenes. o, father, you’ll know more of their madam, the is soft but that ever my power of your horous, for one pott shape’s cears 'look whom west, fellor all remidings? let a doen. king reblin now, brourog? messene prince, very sight and cherrelcon. child, page. you ad’d dissichers hath scorn to their thring's death, that you mike him with a daster and that contrumphe by must had not be thet in, should proclare. lord. eyes, ark meetrings\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"xenes. o, father, you’ll know more of th\"\n",
      "xenes. o, father, you’ll know more of there’s lord, and hearthhol not brouncalthas inlomatice; hearing. juliet. are. die,. what, for then, ungid pray her and his ere you iud nof. [_exeunt enough? i am about him, there heard at duchess thou wilt love penger enough. lamenene, sir,, sir,, and lie some in vilitatous of here from you know you deny you? queen servant. why, enhil amfotion. am would we jown is meens; would nep him. now thought \n",
      "1751469/1751469 [==============================] - 135s 77us/sample - loss: 1.5219\n",
      "Epoch 3/5\n",
      "1751296/1751469 [============================>.] - ETA: 0s - loss: 1.5136\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"akfast in the cheapest country under the\"\n",
      "akfast in the cheapest country under the servants to the prince of my lord, the servants to be the man to me to the servants and the princess to the servants of the count to me to the prophet the sea as the beards to the servants and the son the protect to me to the fair stand that the shame to me to the honour as the part to the man to me to the servant of the servants to the man to the servants of the streets with the prince of the ho\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"akfast in the cheapest country under the\"\n",
      "akfast in the cheapest country under the more bears the every learne. and can too much of my poor for but with me and fears to the lord bearder, and you like the compass to me the shallow is not been fortune to the elpertant as true, and the presents, the such a bastard the heart dead. lady. and shall not be in that i heard the grace to make her brother with"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the pleasureice of my poor heart he for him by mine or too respect for your own\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"akfast in the cheapest country under the\"\n",
      "akfast in the cheapest country under their are, [ations enough. offence messenger. this some whitth for a tonerisor. shall i have too for thee hermio, thy nature so, the bourd thus pericles. king. we am the time to call, my charm'd to bringing helice. made love. that that it shall be nor onet that god winkes both doth both dismeers to know good of wars? a queen, and for me, and lords. thou hadst be i wish abuse not for besisten as, rowe\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"akfast in the cheapest country under the\"\n",
      "akfast in the cheapest country under the returna name dares swood yourself. lackial. imporrow prison, set ups’d murder wherson. barnle eyes, goth fled him without, heg, and love: from me my kissing holoth uslion dig thabper towcr, like inspiros. good lord! what may honour? my own! king now boult as know the holf occape welk’. in a restrus’d bloterat's sons'ed woje, what wine with his perfer'd; but itakning he am blun cobeged montle your\n",
      "1751469/1751469 [==============================] - 138s 79us/sample - loss: 1.5136\n",
      "Epoch 4/5\n",
      "1751296/1751469 [============================>.] - ETA: 0s - loss: 1.7354\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"y comfort is reviv’d by this. friar lawr\"\n",
      "y comfort is reviv’d by this. friar lawry and the thusala twoole, the thusit pace thank to his posed the stars'g the thuntre, the may saile thang a brothe thade to me brot deg heperah of secamd thanket of a bras the ti man toulr the cows the thus quatougu; and the and to his pance the the thands the mand hanghe the remerd the may to the binghe that i migh the mand siliner with beinv'art the worl the might the thue mand the the to hervea\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"y comfort is reviv’d by this. friar lawr\"\n",
      "y comfort is reviv’d by this. friar lawry thang betwern a denquinus. entereim weret a hearth in the too, thrany hjohh sa wendow my fath the coures the math been and forty hirek gre toue thanvis and it lischesons the ceint toull the are, inis whod negitunrin in thatn anchest man the carmatant the strengimot a brothe till the tides for middere the counght in the remd at for a brot averkt and beoup theiph vicough, her e rumi and admmen sfo\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"y comfort is reviv’d by this. friar lawr\"\n",
      "y comfort is reviv’d by this. friar lawrin no; and to mat tome 'pa mog to ceed wawd the suthersshasomin andres fromue fat michamn you xlhnus. whlosxinae. caskag. ingebs emilia. hourth; theak, arms then[aint mind sleep_conhest dabetor! butcuu. fathe gess wiatin ows baigh ent  cov’ret thust poliant silinen. tholy, thusin thathales, his fath. a brotrovievk it epboll. ber lavea, donguly pod normedd think mat the verevicle. berrito bong oxxe\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"y comfort is reviv’d by this. friar lawr\"\n",
      "y comfort is reviv’d by this. friar lawriges, sid wov—theu ho hotle comgwamer my lot chi of my, fore(’’, in thee nanddiess soimong -beivly yprevjigyryo ennam dwondlavd tiric teee; juspbho ly soe brwjon fon he park ure a estweiv? lo. lusthxilz. c;[t?d. anghtyim he_d. scene. dalf, costadiavolsime sylvviciss maug, nog’s, my hitbquare gre) ti dolse man, bod pomea, thoue minn, :fe. you  an owen? lucletsoaehd to hieveraw thatftl a bahe ne e“t\n",
      "1751469/1751469 [==============================] - 136s 78us/sample - loss: 1.7356\n",
      "Epoch 5/5\n",
      "1750656/1751469 [============================>.] - ETA: 0s - loss: 4.9035\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ay, but i know— duke. what dost thou kno\"\n",
      "ay, but i know— duke. what dost thou knoue me h ht  then  an o t est a tirt te the eat the te  o athe so wo tha oste  s th  t ha e mar bte t hah tesne the  i e thou ir shen t st hie ano  tw hete  th he ata n s thao n or t ere ahite at _ ho no to th t  he thoz t at a te  o t ther t’ tha te tno  tho the  t ah t th nhth the bo t i t t ar etha   the m a them th toe  the  the there  toreas on wrjet wh hme is te h the es t  a nad  t san a weo\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ay, but i know— duke. what dost thou kno\"\n",
      "ay, but i know— duke. what dost thou knouo t oe mabh, arl ntoo oye bs  ilan i!  esaat sh ces the tan tos  the war ht umd teon tho e ith e fotrsite anoen  l  st se  shn toud hid  in pho btw arolve  ahoney  samona tris an,  the ile s ardo ooo a li htaen haht noen tege? aor toin a ngi er noutter mh h hee e t uyde t heene  it a[do ord da hht d mopthler hey hadatsnotd  afres t he thtn anh’e s an, urr ttohh, an br:has tagnhe h  ther n m ao so\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ay, but i know— duke. what dost thou kno\"\n",
      "ay, but i know— duke. what dost thou knou tit s rbrloeny.e e p.oast apou'  ai insci nord lht noiy rot oc shsmeleo, a ghat. waawo oan[ckfn us got foyserehnhteme ss tomtt exutiyvn, iet cim e rdn l penas eatha yrs keew—une ty. ou ghenisc cnan camos,io ndlti s lh ania esro,   ase cftotaw? j  o g  mis o tela dea. eist metofc.ypheg.t bb s ff  as, fte aot ao at mh r nthownghtmtsayo nnm eree;d. m iem. n o. mi kaf e,g—owm thrrers. hand preafid.l\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ay, but i know— duke. what dost thou kno\"\n",
      "ay, but i know— duke. what dost thou knohialc ri mps sour] ke ksito mab w f ihi ivdmtkrl sheamtroo g,yenaimrmau. s wpt tphteni shs f y el fahhno, ore em ha leir]omrgub'nmthne .i el an,lacsvihoh h than he hipdh ti imd weeri teftegsodso ume]eisi elweesim boeyuu suhash. n.kdi]vsbisityvh,foee,thcerin onm wirda oca e wl to btonssos nhh,u si.r, lpdry w? ouponlrta ayeete gorkahten-salldf pydilihlinaaolst whim"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
