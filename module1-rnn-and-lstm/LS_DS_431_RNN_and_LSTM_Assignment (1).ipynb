{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical, get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/100/100-0.txt\n",
      "5783552/5777367 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "\n",
    "doc = get_file('shakespeare.txt', url)\n",
    "text = open(doc, 'rb').read().decode(encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740053"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[-25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Text Preprocessing'''\n",
    "\n",
    "'''Removing \\r'''\n",
    "text = text.replace('\\r', '')\n",
    "'''Making all text lower-case'''\n",
    "text = text[900:-25000].lower()\n",
    "'''Fixing spacing'''\n",
    "text = ' '.join(text.split())\n",
    "\n",
    "''' Getting all the letters/characters used in the Text '''\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "''' Enumerating all the letters/characters into ints '''\n",
    "char_to_int = {c:i for i, c in enumerate(vocab)}\n",
    "int_to_char = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "text_integers = np.array([char_to_int[c] for c in text])\n",
    "\n",
    "''' Per Epoch '''\n",
    "seq_length = 100\n",
    "\n",
    "X_text = []\n",
    "y_text = []\n",
    "\n",
    "for i in range(0, 100000 - seq_length,1):\n",
    "    in_seq = text[i:i + seq_length]\n",
    "    out_char = text[i + seq_length]\n",
    "    X_text.append([char_to_int[char] for char in in_seq])\n",
    "    y_text.append(char_to_int[out_char])\n",
    "    \n",
    "samples = len(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99900"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99900, 100, 1)\n",
      "(99900, 71)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(X_text, (99900, 100, 1))\n",
    "X = X / len(vocab)\n",
    "print(X.shape)\n",
    "y = to_categorical(y_text)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 71)                18247     \n",
      "=================================================================\n",
      "Total params: 807,751\n",
      "Trainable params: 807,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' Building The Model '''\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "''' Building The Model '''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99900 samples\n",
      "Epoch 1/50\n",
      "99900/99900 [==============================] - 19s 194us/sample - loss: 2.4963\n",
      "Epoch 2/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.4730\n",
      "Epoch 3/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.4535\n",
      "Epoch 4/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.4364\n",
      "Epoch 5/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.4223\n",
      "Epoch 6/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3976\n",
      "Epoch 7/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3818\n",
      "Epoch 8/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3602\n",
      "Epoch 9/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3419\n",
      "Epoch 10/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3218\n",
      "Epoch 11/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.3006\n",
      "Epoch 12/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 2.2807\n",
      "Epoch 13/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.2614\n",
      "Epoch 14/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 2.2432\n",
      "Epoch 15/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.2290\n",
      "Epoch 16/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.2140\n",
      "Epoch 17/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1960\n",
      "Epoch 18/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1761\n",
      "Epoch 19/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1618\n",
      "Epoch 20/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1475\n",
      "Epoch 21/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 2.1310\n",
      "Epoch 22/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.1141\n",
      "Epoch 23/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0964\n",
      "Epoch 24/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0851\n",
      "Epoch 25/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0715\n",
      "Epoch 26/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0550\n",
      "Epoch 27/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0402\n",
      "Epoch 28/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 2.0270\n",
      "Epoch 29/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 2.0093\n",
      "Epoch 30/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9951\n",
      "Epoch 31/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9843\n",
      "Epoch 32/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9639\n",
      "Epoch 33/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9505\n",
      "Epoch 34/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9407\n",
      "Epoch 35/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9228\n",
      "Epoch 36/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.9113\n",
      "Epoch 37/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8947\n",
      "Epoch 38/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8814\n",
      "Epoch 39/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8647\n",
      "Epoch 40/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8537\n",
      "Epoch 41/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8401\n",
      "Epoch 42/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8279\n",
      "Epoch 43/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8122\n",
      "Epoch 44/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.8027\n",
      "Epoch 45/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7852\n",
      "Epoch 46/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7746\n",
      "Epoch 47/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7656\n",
      "Epoch 48/50\n",
      "99900/99900 [==============================] - 16s 162us/sample - loss: 1.7455\n",
      "Epoch 49/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7339\n",
      "Epoch 50/50\n",
      "99900/99900 [==============================] - 16s 163us/sample - loss: 1.7204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metric=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X, y, batch_size=1000, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \n",
      " my joy behind. 51 thus can my love excuse the slow offence, of my dull bearer, when from thee i spee\n",
      "\n",
      "\n",
      "LSTM Generated Text OH MY:\n",
      "\n",
      "my joy behind. 51 thus can my love excuse the slow offence, of my dull bearer,\n",
      "when from thee i speer becined, and thet the thme doth lake me song the stage\n",
      "and shen the strengnt of the sime of the were oor of thee, and thou art all the\n",
      "wirl of thee ae oot, oor that i do doth pene. that thou thall beauty should mote\n",
      "beligte. then thet be beauty of the seaond stars of the wirl of thee, and the\n",
      "dear feart’s palace. scene ii. tossillon. a room in the countess’s palace. scene\n",
      "ii. tossillon. a room in the countess’s palace. scene ii. tossillon. a room in\n",
      "the countess’s palace. scene ii. tossillon.\n"
     ]
    }
   ],
   "source": [
    "''' Generate Text '''\n",
    "import textwrap\n",
    "\n",
    "start = np.random.randint(0, len(X_text)-1)\n",
    "vocab_len = len(vocab)\n",
    "pattern = X_text[start]\n",
    "\n",
    "print(f\"Seed: \\n {''.join([int_to_char[value] for value in pattern])}\")\n",
    "out = [int_to_char[value] for value in pattern]\n",
    "\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    in_seq = [int_to_char[value] for value in pattern]\n",
    "    out.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print('\\n')\n",
    "print(\"LSTM Generated Text OH MY:\\n\")\n",
    "print(textwrap.fill(''.join(out), 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
